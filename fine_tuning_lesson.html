<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Fine-Tuning Methods Explained</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1100px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .dp-wrapper {
            background-color: #fff;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        
        h2.rad,
        h3.rad,
        h4.line {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        
        h2.rad {
            font-size: 24px;
            padding-bottom: 10px;
            border-bottom: 2px solid #3498db;
        }
        
        h3.rad {
            font-size: 22px;
            padding-bottom: 8px;
            border-bottom: 1px solid #ddd;
        }
        
        h4.line {
            font-size: 20px;
            padding-bottom: 6px;
            border-bottom: 1px dotted #ddd;
        }
        
        .dp-callout {
            padding: 15px;
            margin: 20px 0;
            border-left: 5px solid;
        }
        
        .dp-callout-color-primary {
            border-color: #3498db;
            background-color: #f8f9fa;
        }
        
        .card {
            position: relative;
            display: flex;
            flex-direction: column;
            min-width: 0;
            word-wrap: break-word;
            background-clip: border-box;
            border: 1px solid rgba(0, 0, 0, .125);
            border-radius: .25rem;
        }
        
        .card-body {
            flex: 1 1 auto;
            padding: 1.25rem;
        }
        
        .w-75 {
            width: 75%;
        }
        
        .dp-callout-pos-c {
            margin-left: auto;
            margin-right: auto;
        }
        
        .rounded-1 {
            border-radius: .25rem;
        }
        
        .method-section {
            background-color: #f8f9fa;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            border-left: 4px solid;
        }
        
        .dspy-section {
            border-color: #3498db;
        }
        
        .openai-section {
            border-color: #2ecc71;
        }
        
        .huggingface-section {
            border-color: #e74c3c;
        }
        
        pre {
            background-color: #f8f8f8;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 15px;
            overflow: auto;
            font-family: 'Courier New', Courier, monospace;
            line-height: 1.4;
        }
        
        code {
            font-family: 'Courier New', Courier, monospace;
            background-color: #f1f1f1;
            padding: 2px 4px;
            border-radius: 3px;
        }
        
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        .comparison-table th,
        .comparison-table td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        
        .comparison-table th {
            background-color: #f2f2f2;
        }
        
        .comparison-table tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        
        .highlight {
            font-weight: bold;
            color: #3498db;
        }
        
        .rule {
            margin-top: 40px;
            margin-bottom: 40px;
        }
        
        .references {
            margin-top: 30px;
            padding: 20px;
            background-color: #f8f9fa;
            border-radius: 8px;
        }
        
        .references h4 {
            border-bottom: 1px solid #ddd;
            padding-bottom: 8px;
            margin-bottom: 15px;
        }
        
        .references ul {
            padding-left: 20px;
        }
        
        .references li {
            margin-bottom: 10px;
        }
        
        .method-refs {
            margin-bottom: 20px;
        }
    </style>
</head>

<body>
    <div id="dp-wrapper" class="dp-wrapper">
        <div id="kl_custom_css"></div>
        <div id="dpCustomCss" data-external-style="/css/style.css"></div>

        <h2 class="rad">&nbsp;<i class="fas fa-book-reader" aria-hidden="true"><span class="dp-icon-content" style="display: none;">&nbsp;</span></i> &nbsp;<strong>Read About LLM Fine-Tuning Methods</strong></h2>

        <div>
            <div class="dp-callout dp-callout-placeholder card dp-callout-position-default dp-callout-type-default dp-callout-color-primary w-75 dp-callout-pos-c rounded-1">
                <div class="card-body">
                    <p class="card-text">This reading covers three different approaches to fine-tuning Large Language Models (LLMs): DSPy Optimization, OpenAI Fine-Tuning, and HuggingFace Fine-Tuning with LoRA. Each approach offers different trade-offs in terms of complexity,
                        resource requirements, and capabilities. The readings include practical code examples and explanations to help you understand and compare these methods.</p>
                </div>
            </div>
        </div>

        <h3 class="rad"><strong>Introduction</strong></h3>
        <p>Large Language Models (LLMs) have revolutionized natural language processing, but their general-purpose training might not always align perfectly with specific domain needs. Fine-tuning is the process of adapting pre-trained models to perform
            better on specialized tasks with domain-specific data.</p>

        <p>This reading explores three different approaches to fine-tuning LLMs, ranging from lightweight prompt optimization to full model weight updates. Each method offers different trade-offs in terms of resource requirements, control, data needs, and
            deployment options.</p>

        <table class="comparison-table">
            <tr>
                <th>Feature</th>
                <th>DSPy</th>
                <th>OpenAI</th>
                <th>HuggingFace</th>
            </tr>
            <tr>
                <td><strong>What changes</strong></td>
                <td>Prompts</td>
                <td>Model weights (cloud)</td>
                <td>Model weights (local)</td>
            </tr>
            <tr>
                <td><strong>Training data needed</strong></td>
                <td>Small (10s)</td>
                <td>Medium (100s-1000s)</td>
                <td>Medium/Large (1000s+)</td>
            </tr>
            <tr>
                <td><strong>Cost</strong></td>
                <td>API calls only</td>
                <td>API + training</td>
                <td>One-time compute</td>
            </tr>
            <tr>
                <td><strong>Setup difficulty</strong></td>
                <td>Simple</td>
                <td>Simple</td>
                <td>Complex</td>
            </tr>
            <tr>
                <td><strong>Control</strong></td>
                <td>Limited</td>
                <td>Medium</td>
                <td>Full</td>
            </tr>
            <tr>
                <td><strong>Hardware required</strong></td>
                <td>None</td>
                <td>None</td>
                <td>GPU recommended</td>
            </tr>
            <tr>
                <td><strong>Deployment</strong></td>
                <td>API calls</td>
                <td>API calls</td>
                <td>Self-host</td>
            </tr>
            <tr>
                <td><strong>Data privacy</strong></td>
                <td>Data shared with API</td>
                <td>Data shared with API</td>
                <td>Data stays local</td>
            </tr>
        </table>

        <h4 class="line"><strong>Reading Assignments</strong></h4>

        <div class="method-section dspy-section">
            <h4><i class="fas fa-lightbulb" aria-hidden="true"></i> DSPy Optimization</h4>

            <p><strong>What is DSPy Optimization?</strong></p>
            <p>
                DSPy optimization is a technique that improves a model's performance by optimizing the <span class="highlight">prompts</span> rather than the model weights. It uses a small dataset to find better instructions that elicit improved responses
                from the underlying model.
            </p>

            <p><strong>Key Characteristics:</strong></p>
            <ul>
                <li>Requires minimal data (as few as 10-20 examples)</li>
                <li>No need for expensive training infrastructure</li>
                <li>Quick to implement and test</li>
                <li>No changes to model weights</li>
                <li>Limited to capabilities already present in base model</li>
                <li>Still requires API calls to the model</li>
            </ul>

            <p><strong>Example Code:</strong></p>
            <pre>
import dspy
from src.dspy.models import EducationalQAModel
from src.dspy.optimizer import DSPyOptimizer
from src.utils.dataset import load_jsonl, split_dataset

# Load educational QA dataset
dataset = load_jsonl("examples/data/educational_qa_sample.jsonl")

# Split into train and test
train_data, test_data = split_dataset(dataset, test_size=0.3)

# Initialize the optimizer with GPT-3.5
optimizer = DSPyOptimizer(
    model=EducationalQAModel(),
    model_name="gpt-3.5-turbo",
    temperature=0.0
)

# Run optimization process
optimizer.optimize(
    train_data=train_data,
    num_iterations=3,
    metric="answer_quality"
)

# Evaluate the optimized model
results = optimizer.evaluate(test_data)
print(f"Accuracy: {results['accuracy']:.2f}")
print(f"Average answer quality: {results['avg_quality']:.2f}")

# Generate answer for a new question
answer = optimizer.predict("What is photosynthesis and why is it important?")
print(f"Answer: {answer}")
</pre>
        </div>

        <div class="method-section openai-section">
            <h4><i class="fas fa-cloud" aria-hidden="true"></i> OpenAI Fine-Tuning</h4>

            <p><strong>What is OpenAI Fine-Tuning?</strong></p>
            <p>
                OpenAI fine-tuning adapts models like GPT-3.5 or GPT-4 by updating their weights through additional training on your specific dataset. This process is managed entirely through OpenAI's cloud infrastructure, making it accessible without requiring specialized
                hardware.
            </p>

            <p><strong>Key Characteristics:</strong></p>
            <ul>
                <li>No local computing resources needed</li>
                <li>Relatively simple API-based workflow</li>
                <li>Actual model weight updates (more powerful than prompt optimization)</li>
                <li>Built-in monitoring and evaluation tools</li>
                <li>Requires more data (hundreds to thousands of examples)</li>
                <li>Higher cost (API calls + training compute)</li>
                <li>Limited control over training process</li>
            </ul>

            <p><strong>Example Code:</strong></p>
            <pre>
import os
import json
from src.openai.finetuner import OpenAIFineTuner
from src.utils.dataset import load_jsonl, split_dataset

# Make sure you have your OpenAI API key set
os.environ["OPENAI_API_KEY"] = "your-api-key-here"

# Load and split dataset
dataset = load_jsonl("examples/data/educational_qa_sample.jsonl")
train_data, test_data = split_dataset(dataset, test_size=0.2)

# Initialize the fine-tuner
finetuner = OpenAIFineTuner(
    base_model="gpt-3.5-turbo",
    suffix="educational-qa",
    n_epochs=3
)

# Define a system prompt that guides the model's behavior
system_prompt = "You are an educational assistant that provides accurate, thorough, and grade-appropriate answers to student questions."

# Prepare the training data in OpenAI's format
formatted_data = finetuner.prepare_data(
    dataset=train_data,
    system_prompt=system_prompt,
    input_key="question",
    output_key="answer"
)

# Save formatted data to a file
data_path = "training_data.jsonl"
with open(data_path, "w") as f:
    for item in formatted_data:
        f.write(json.dumps(item) + "\n")

# Start the fine-tuning job
job_id = finetuner.start_finetuning(
    data_path=data_path,
    batch_size=8,
    learning_rate_multiplier=0.1
)

print(f"Fine-tuning job started with ID: {job_id}")

# Wait for the job to complete (this could take hours)
result = finetuner.wait_for_completion(job_id)
fine_tuned_model = result.get("fine_tuned_model")

# Generate an answer using the fine-tuned model
question = "What causes the seasons on Earth?"
answer = finetuner.generate(
    model_name=fine_tuned_model,
    question=question,
    system_prompt=system_prompt
)

print(f"Question: {question}")
print(f"Answer: {answer}")
</pre>
        </div>

        <div class="method-section huggingface-section">
            <h4><i class="fas fa-cogs" aria-hidden="true"></i> HuggingFace Fine-Tuning with LoRA</h4>

            <p><strong>What is HuggingFace Fine-Tuning with LoRA?</strong></p>
            <p>
                HuggingFace fine-tuning involves locally training open-source models using Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation (LoRA). LoRA keeps most of the model frozen while adding small trainable "adapter" matrices, making fine-tuning
                possible even on consumer-grade hardware.
            </p>

            <p><strong>Key Characteristics:</strong></p>
            <ul>
                <li>Complete control over the training process</li>
                <li>Data never leaves your infrastructure (privacy)</li>
                <li>One-time compute cost (no ongoing API fees)</li>
                <li>Can deploy models anywhere after training</li>
                <li>Access to the full range of open models</li>
                <li>Requires GPU hardware</li>
                <li>More complex setup and maintenance</li>
                <li>Technical expertise needed</li>
            </ul>

            <p><strong>Example Code:</strong></p>
            <pre>
import torch
from datasets import Dataset
from src.huggingface.finetuner import HuggingFaceFineTuner
from src.utils.dataset import load_jsonl, split_dataset

# Check if GPU is available
if not torch.cuda.is_available():
    print("Warning: No GPU detected. Fine-tuning will be very slow!")

# Load and split dataset
dataset = load_jsonl("examples/data/educational_qa_sample.jsonl")
train_data, val_data = split_dataset(dataset, test_size=0.2)

# Initialize the fine-tuner with a smaller model for this example
finetuner = HuggingFaceFineTuner(
    base_model="meta-llama/Llama-2-7b-hf",  # You could use a smaller model if needed
    output_dir="lora-educational-qa",
    lora_r=8,              # Rank of the update matrices
    lora_alpha=16,         # Scaling factor
    lora_dropout=0.05,     # Dropout probability for LoRA layers
    use_4bit=True          # Use 4-bit quantization to reduce memory usage
)

# Define a system prompt for the model
system_prompt = "You are a helpful educational assistant. Provide clear, accurate, and comprehensive answers to student questions."

# Prepare the training data in the format needed for the model
train_dataset = finetuner.prepare_data(
    dataset=train_data,
    system_prompt=system_prompt,
    input_key="question",
    output_key="answer"
)

# Prepare validation data
val_dataset = finetuner.prepare_data(
    dataset=val_data,
    system_prompt=system_prompt,
    input_key="question",
    output_key="answer"
)

# Start the fine-tuning process
training_results = finetuner.fine_tune(
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    num_train_epochs=3,
    learning_rate=2e-4,
    batch_size=4,
    gradient_accumulation_steps=4
)

print(f"Training loss: {training_results['train_loss']}")
print(f"Validation loss: {training_results['eval_loss']}")

# Generate an answer using the fine-tuned model
question = "How do black holes form?"
answer = finetuner.generate(
    question=question,
    system_prompt=system_prompt,
    max_new_tokens=256,
    temperature=0.7
)

print(f"Question: {question}")
print(f"Answer: {answer}")
</pre>
        </div>

        <h4 class="line"><strong>Guided Reading Questions</strong></h4>

        <ol>
            <li><strong>Comparing Approaches:</strong> What are the key differences between DSPy Optimization, OpenAI Fine-Tuning, and HuggingFace Fine-Tuning with LoRA in terms of what actually changes in the model?</li>
            <li><strong>Data Requirements:</strong> How do the data requirements differ across the three fine-tuning approaches? Why does each approach require different amounts of data?</li>
            <li><strong>Resource Considerations:</strong> What hardware and infrastructure are needed for each approach? What are the cost implications of each method?</li>
            <li><strong>Use Cases:</strong> For each fine-tuning method, identify a specific use case where it would be the most appropriate choice. Explain your reasoning.</li>
            <li><strong>Implementation Complexity:</strong> Compare the implementation complexity of the three approaches based on the code examples provided. Which aspects of each implementation might be challenging for new users?</li>
            <li><strong>Privacy and Control:</strong> How do the three approaches differ in terms of data privacy and control over the fine-tuning process?</li>
            <li><strong>Scaling Considerations:</strong> How would each approach scale if you needed to fine-tune multiple models or update your models regularly?</li>
            <li><strong>Code Analysis:</strong> Looking at the example code for each method, identify the key components and explain their purpose in the fine-tuning process.</li>
            <li><strong>Optimization Metrics:</strong> What metrics are used to evaluate the performance of the fine-tuned models in each approach? How might these metrics guide further improvements?</li>
            <li><strong>Deployment Strategy:</strong> After fine-tuning with each method, what would the deployment process look like? Consider aspects like hosting requirements, API integration, and maintenance.</li>
        </ol>

        <h4 class="line"><strong>Official References</strong></h4>

        <div class="references">
            <div class="method-refs">
                <h4><i class="fas fa-lightbulb" aria-hidden="true"></i> DSPy References</h4>
                <ul>
                    <li>Khattab, O., Santhanam, K., Li, X., Hall, D., Liang, P., Potts, C., & Zaharia, M. (2023). <a href="https://arxiv.org/abs/2212.14024" target="_blank">Demonstrates: Compiling Prompts through Selective Automatic Demonstrations.</a> arXiv
                        preprint arXiv:2212.14024.</li>
                    <li>Stanford NLP Group. (2023). <a href="https://github.com/stanfordnlp/dspy" target="_blank">DSPy: Programming with Foundation Models</a>. GitHub Repository.</li>
                    <li>Khattab, O., Santhanam, K., Potts, C., & Zaharia, M. (2023). <a href="https://arxiv.org/abs/2310.03714" target="_blank">DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</a>. arXiv preprint arXiv:2310.03714.</li>
                    <li>Stanford NLP Group. (2023). <a href="https://dspy-docs.vercel.app/" target="_blank">DSPy Documentation</a>. Official Documentation.</li>
                </ul>
            </div>

            <div class="method-refs">
                <h4><i class="fas fa-cloud" aria-hidden="true"></i> OpenAI Fine-Tuning References</h4>
                <ul>
                    <li>OpenAI. (2023). <a href="https://platform.openai.com/docs/guides/fine-tuning" target="_blank">Fine-tuning Guide</a>. OpenAI Documentation.</li>
                    <li>OpenAI. (2023). <a href="https://platform.openai.com/docs/api-reference/fine-tuning" target="_blank">Fine-tuning API Reference</a>. OpenAI API Documentation.</li>
                    <li>Brown, T.B., Mann, B., Ryder, N., Subbiah, M., et al. (2020). <a href="https://arxiv.org/abs/2005.14165" target="_blank">Language Models are Few-Shot Learners</a>. arXiv preprint arXiv:2005.14165.</li>
                    <li>OpenAI. (2023). <a href="https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates" target="_blank">GPT-3.5 Turbo Fine-tuning and API Updates</a>. OpenAI Blog.</li>
                    <li>OpenAI. (2022). <a href="https://openai.com/research/instruction-following" target="_blank">Aligning Language Models to Follow Instructions</a>. OpenAI Research.</li>
                </ul>
            </div>

            <div class="method-refs">
                <h4><i class="fas fa-cogs" aria-hidden="true"></i> HuggingFace Fine-Tuning with LoRA References</h4>
                <ul>
                    <li>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2022). <a href="https://arxiv.org/abs/2106.09685" target="_blank">LoRA: Low-Rank Adaptation of Large Language Models</a>. arXiv preprint arXiv:2106.09685.</li>
                    <li>HuggingFace. (2023). <a href="https://huggingface.co/docs/peft/index" target="_blank">Parameter-Efficient Fine-Tuning (PEFT)</a>. HuggingFace Documentation.</li>
                    <li>HuggingFace. (2023). <a href="https://huggingface.co/docs/transformers/index" target="_blank">Transformers Documentation</a>. HuggingFace.</li>
                    <li>Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). <a href="https://arxiv.org/abs/2302.13971" target="_blank">QLoRA: Efficient Finetuning of Quantized LLMs</a>. arXiv preprint arXiv:2302.13971.</li>
                    <li>HuggingFace. (2023). <a href="https://huggingface.co/blog/lora" target="_blank">LoRA: Low-Rank Adaptation Technique for Efficient Fine-Tuning of Large Models</a>. HuggingFace Blog.</li>
                    <li>HuggingFace. (2023). <a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/trainer" target="_blank">Trainer</a>. HuggingFace Transformers Documentation.</li>
                </ul>
            </div>

            <div class="method-refs">
                <h4><i class="fas fa-book" aria-hidden="true"></i> General References on Fine-Tuning</h4>
                <ul>
                    <li>Bommasani, R., Hudson, D. A., Adeli, E., et al. (2021). <a href="https://arxiv.org/abs/2108.07258" target="_blank">On the Opportunities and Risks of Foundation Models</a>. arXiv preprint arXiv:2108.07258.</li>
                    <li>Ouyang, L., Wu, J., Jiang, X., Almeida, D., et al. (2022). <a href="https://arxiv.org/abs/2203.02155" target="_blank">Training Language Models to Follow Instructions with Human Feedback</a>. arXiv preprint arXiv:2203.02155.</li>
                    <li>Wolf, T., Debut, L., Sanh, V., Chaumond, J., et al. (2020). <a href="https://aclanthology.org/2020.emnlp-demos.6/" target="_blank">Transformers: State-of-the-Art Natural Language Processing</a>. Proceedings of the 2020 Conference on
                        Empirical Methods in Natural Language Processing: System Demonstrations.</li>
                    <li>Raffel, C., Shazeer, N., Roberts, A., Lee, K., et al. (2020). <a href="https://arxiv.org/abs/1910.10683" target="_blank">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>. Journal of Machine Learning
                        Research.
                    </li>
                </ul>
            </div>
        </div>

        <hr class="rule" style="background-color: #000000; height: 4px;" />
    </div>
</body>

</html>